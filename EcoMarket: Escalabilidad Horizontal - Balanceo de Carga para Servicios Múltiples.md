---

## 2. Archivo: `INFORME_HITO_2.md`

Copia todo el contenido dentro de este bloque y p칠galo en un archivo nuevo llamado `INFORME_HITO_2.md` en la misma carpeta.

```markdown
# 游늯 Informe T칠cnico: Hito 2 - Escalabilidad y Resiliencia en EcoMarket

**Autor:** [Tu Nombre]
**Fecha:** 5 de noviembre de 2025

---

## 1. Justificaci칩n de Escalabilidad (Horizontal vs. Vertical)

Para la evoluci칩n de la API Central de EcoMarket, se opt칩 por una estrategia de **escalabilidad horizontal** (escalar "hacia afuera") en lugar de una vertical (escalar "hacia arriba").

* **Escalabilidad Vertical** implica aumentar los recursos de una sola m치quina (m치s CPU, m치s RAM). Aunque es simple de implementar inicialmente, tiene un l칤mite f칤sico, es costoso y presenta un **punto 칰nico de fallo** (SPOF). Si esa 칰nica m치quina falla, todo el servicio de la central se cae.

* **Escalabilidad Horizontal**, la estrategia implementada, implica a침adir m치s instancias del servicio (m치s contenedores) y distribuir la carga entre ellas.

### Ventajas de la Escalabilidad Horizontal (Lo que logramos):

1.  **Alta Disponibilidad (Resiliencia):** Esta es la ventaja principal. Al tener dos instancias (`central-api-1` y `central-api-2`), si una de ellas falla o se detiene para mantenimiento, el balanceador de carga Nginx la detecta y redirige autom치ticamente todo el tr치fico a la instancia saludable. **El servicio nunca se interrumpe para el usuario**, como se demuestra en el video de prueba.

2.  **Mayor Throughput (Rendimiento):** Podemos manejar un mayor n칰mero de peticiones simult치neas. Si una sola instancia pod칤a manejar 100 peticiones/segundo, dos instancias pueden manejar (te칩ricamente) 200 peticiones/segundo. La carga se reparte, evitando que una sola instancia se sature.

3.  **Costo-Efectividad y Flexibilidad:** Es generalmente m치s barato a침adir m칰ltiples "m치quinas" peque침as que mantener una sola m치quina "gigante". Podemos escalar de 2 a 5 instancias durante picos de demanda (como el Black Friday) y volver a 2 en horas valle.

### Retos Abordados:

El principal reto de la escalabilidad horizontal es la **gesti칩n del estado**. Si cada API tuviera su propia base de datos, el sistema ser칤a inconsistente.

Lo solucionamos de la siguiente manera:
* **Estado de Base de Datos (Inventario, Ventas, Usuarios):** Se centraliz칩 en **Redis**. Ambas instancias de la API se conectan a la *misma* base de datos de Redis, asegurando que ambas vean el mismo stock y la misma lista de usuarios.
* **Estado de Tareas (Notificaciones):** Se desacopl칩 usando **RabbitMQ**. Cuando una sucursal env칤a una venta (Modo 6 - Fanout), RabbitMQ la entrega a *ambas* instancias. Para evitar que ambas procesen la misma venta (duplicando el descuento de stock o el contador de usuarios), se implement칩 **idempotencia** usando un "lock" en Redis (`sale_lock:` y `user_event_lock:`), asegurando que solo la primera instancia en recibir el mensaje lo procese.

## 2. Distribuci칩n Lograda (Resultados)

Se implement칩 un balanceador de carga Nginx que act칰a como proxy reverso para las dos instancias de la API Central.

* **Algoritmo Utilizado:** `Round Robin` (el algoritmo por defecto de Nginx). Este m칠todo distribuye cada nueva petici칩n a la siguiente instancia en la lista, en un ciclo.
* **Evidencia (Logs):** Al refrescar el dashboard de la central (`http://localhost/dashboard`) repetidamente, los logs de Docker muestran claramente la alternancia:

    ```bash
    central-api-1 | INFO:  172.18.0.7:55930 - "GET /dashboard HTTP/1.0" 200 OK
    central-api-2 | INFO:  172.18.0.7:49522 - "GET /dashboard HTTP/1.0" 200 OK
    central-api-1 | INFO:  172.18.0.7:55946 - "GET /dashboard HTTP/1.0" 200 OK
    central-api-2 | INFO:  172.18.0.7:49536 - "GET /dashboard HTTP/1.0" 200 OK
    ```

* **Prueba de Fallo:** Al ejecutar `docker stop central-api-1`, se observ칩 que el dashboard segu칤a funcionando sin errores. Los logs mostraron que el 100% de las peticiones se redirig칤an instant치neamente a `central-api-2`, validando la configuraci칩n de alta disponibilidad.

## 3. Mejoras Futuras

Aunque el sistema actual es robusto, se pueden implementar las siguientes mejoras:

1.  **Auto-scaling:** Utilizar un orquestador m치s avanzado como **Kubernetes** o **Docker Swarm** para monitorear la carga (CPU/RAM) de las instancias y autom치ticamente "escalar" (a침adir m치s contenedores) durante picos de demanda y "desescalar" (eliminarlos) cuando la demanda baje.

2.  **M칠tricas y Monitoreo:** Implementar **Prometheus** y **Grafana**. Prometheus recolectar칤a m칠tricas detalladas (n칰mero de peticiones/seg, tasa de errores 5xx, latencia, longitud de la cola en RabbitMQ) y Grafana las mostrar칤a en dashboards visuales para monitorear la salud del sistema en tiempo real.

3.  **Algoritmo de Balanceo Avanzado:** En producci칩n, cambiar de `Round Robin` a `least_conn` (Menos Conexiones). Este algoritmo env칤a la nueva petici칩n a la instancia que tenga el menor n칰mero de conexiones activas, siendo m치s eficiente si algunas peticiones son m치s "pesadas" (tardan m치s) que otras.
